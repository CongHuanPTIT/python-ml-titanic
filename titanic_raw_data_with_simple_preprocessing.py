# -*- coding: utf-8 -*-
"""Titanic - Raw Data with Simple Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yw-lgsUZiiXgzUtpy5avPJD2_fTzoEfJ
"""

"""The code below is generated to fetch the neccessary data files from Kaggle"""

from google.colab import files
files.upload()

! rm -r ~/.kaggle
! mkdir ~/.kaggle
! mv ./kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle competitions download titanic

! mkdir /content/titanic

! unzip /content/titanic.zip -d /content/titanic/

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

train_data = pd.read_csv('/content/titanic/train.csv')
test_data = pd.read_csv('/content/titanic/test.csv')

"""A look at the data"""

print(train_data.info())
print(train_data.head(10))

print(test_data.info())
print(test_data.head())

"""Dealing with null (missing values) data in 'Age' and 'Fare' columns. The 'Cabin' column will be dealt with later.

Fill in with mean values."""

train_data.Age.fillna(train_data.Age.mean(), inplace=True)
test_data.Age.fillna(test_data.Age.mean(), inplace=True)

test_data[test_data['Fare'].isna()]

test_data.Fare.fillna(test_data.Fare.mean(), inplace=True)

"""Examine how the feature columns affect the outcome ('Survived' column). This is called "finding correlation"."""

corr = train_data.loc[:, train_data.columns != 'PassengerId'].corr(numeric_only=True)
print(corr) # The corr() function computes the pairwise correlation of columns
     # Higher value means more significant relationships

# Looking at a lot of numbers is confusing... so let's plot it.
# Do I know all of these? No. I took an example from a dataset about diabetes
# (also on Kaggle) and apply the plot code.

col = ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']
plot_data = train_data[col].copy()
fig, ax = plt.subplots(figsize=(6, 6))
cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0,len(plot_data.columns),1)
ax.set_xticks(ticks)
ax.set_xticklabels(plot_data.columns)
plt.xticks(rotation = 90)
ax.set_yticklabels(plot_data.columns)
ax.set_yticks(ticks)
for i in range(train_data.shape[1]):
  for j in range(6):
    text = ax.text(j, i, round(corr.iloc[i][j],4), ha="center", va="center", color="w")
plt.show()

# Okay so... it's not perfect. Error still exists (out-of-bounds) but the
# important thing is the code works, the plot has been drawn and
# we now have the values. From these values, let's get the
# top 3 features with the highest correlation with the 'Survived' outcome.
print(train_data.loc[:, train_data.columns != 'PassengerId'].corr(numeric_only=True).nlargest(4, 'Survived').index)
print(train_data.loc[:, train_data.columns != 'PassengerId'].corr(numeric_only=True).nlargest(4, 'Survived').values[:, 5])

"""So 'Fare', 'Parch' and 'SibSp' are the three most influential featues to the 'Survived' outcome.

However, consider the sample submission (which only counts for gender), there may be more features 
contributing to the outcome. If we use human logic: Young people may have a higher chance to survive 
than old people; men may be more likely to survive than women...

So for this case, let's factor Sex and Age into the game. One problem: The Sex column is in strings. 
Machines can't work with strings.

More preprocessing will be needed.
"""

# One way we can deal with this is by making an "isMale" column, data type integer.
# If the passenger's gender is male, then isMen is 1, else it's 0.
train_data['isMale'] = np.where(train_data['Sex'] == 'male', 1, 0)
test_data['isMale'] = np.where(test_data['Sex'] == 'male', 1, 0)

base_features = ['Fare', 'Parch', 'SibSp', 'Age', 'isMale']

X_train, y_train = train_data[base_features], train_data['Survived'].copy()
X_test = test_data[base_features]

"""Evaluate the best algorithm to use"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
# There are many more "fancy" classification models, but for practice purpose,
# these basic ones will do.
from sklearn.model_selection import cross_val_score

"""Evaluate the algorithms in terms of accuracy, using cross validation. Save the results and compare"""

acc_score = []
logis = LogisticRegression()
logis_score = cross_val_score(logis, X_train, y_train, cv=50, scoring='accuracy').mean()

print(logis_score)
acc_score.append(logis_score)

decision = DecisionTreeClassifier(random_state = 50)
decision_score = cross_val_score(decision, X_train, y_train, cv=50, scoring='accuracy').mean()

print(decision_score)
acc_score.append(decision_score)

knn_cv = KNeighborsClassifier(n_neighbors=30)
knn_score = cross_val_score(knn_cv, X_train, y_train, cv=50, scoring='accuracy').mean()

print(knn_score)
acc_score.append(knn_score)

rdf = RandomForestClassifier(random_state=30)
rdf_score = cross_val_score(rdf, X_train, y_train, cv=50, scoring='accuracy').mean()

print(rdf_score)
acc_score.append(rdf_score)

algorithms = ["Logistic Regression", "Decision Tree", "K Nearest Neighbors", "Random Forest"]
cv_mean = pd.DataFrame(acc_score, index = algorithms)
cv_mean.columns=["Accuracy"]
cv_mean.sort_values(by="Accuracy",ascending=False)
print(cv_mean)

"""With the result above, Random Forest grants us the highest accuracy. So we'll use it to fit our test data and make predictions."""

# The default number of trees in Random Forest (n_estimators) is 100.
# We will use this value for now.

rdf.fit(X_train, y_train)

predictions = rdf.predict(X_test)

output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})

print(output)

